{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15038313.svg)](https://doi.org/10.5281/zenodo.15038313)\n",
    "\n",
    "**Date:** July 2024    \n",
    "**Status:** Ongoing experiment  \n",
    "**People involved:** Leon van Wissen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We trained a Word2Vec model on the GLOBALISE Transcriptions, creating vector representations of words based on their context. By leveraging this model you can:\n",
    "\n",
    "1. **Find Spelling Variants and Synonyms**: Discover alternative spellings or synonyms of a word by identifying those with similar vector representations. This is particularly useful for early modern texts with inconsistent orthography.\n",
    "2. **Contextual Similarity**: Locate words that frequently appear in similar contexts, shedding light on semantic relationships. For instance, the term `plantage` (plantation) might reveal associations with specific crops or geographic regions.\n",
    "3. **Advanced Semantic Queries**: Perform tasks such as analogy generation (e.g., `noten is to banda as [X] is to ceylon`) and compute word similarities. These functionalities help researchers uncover patterns and insights from the corpus that are difficult to detect manually.\n",
    "\n",
    "This notebook guides you through loading and running our pretrained model and provides some examples of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short User Guide\n",
    "\n",
    "**Option 1 (Google Colab):**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/globalise-huygens/lab.globalise.huygens.knaw.nl/blob/main/docs/experiments/GLOBALISE_Word2Vec_Lab.ipynb)\n",
    "\n",
    "1. [Open this notebook in Google Colab](https://colab.research.google.com/github/globalise-huygens/lab.globalise.huygens.knaw.nl/blob/main/docs/experiments/GLOBALISE_Word2Vec_Lab.ipynb).\n",
    "2. Run the cells below to load the model and start querying the corpus.\n",
    "\n",
    "**Option 2 (local):**\n",
    "\n",
    "Download this notebook and our pretrained model, follow the cells below, and start exploring the corpus.\n",
    "\n",
    "**Download links:**\n",
    "* Notebook: https://github.com/globalise-huygens/lab.globalise.huygens.knaw.nl/blob/main/docs/experiments/GLOBALISE_Word2Vec_Lab.ipynb\n",
    "* Pretrained model (100 dimensions, 645MB): https://surfdrive.surf.nl/files/index.php/s/XmUIlsy33vpRdCX\n",
    "Note: download and unzip the `GLOBALISE.word2vec.zip` file in a `data` directory in the same folder as this notebook for the cells below to work. The cells below will do this for you if you have the `wget` and `unzip` commands available.\n",
    "\n",
    "If you haven’t used Jupyter notebooks before, we recommend looking up a user guide online. Anaconda is an easy-to-use package. Make sure you have the required libraries (such as Gensim) installed in the Python environment you’re using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Pretrained Model and install Gensim\n",
    "The cell below downloads the pretrained model and installs the Gensim library. If you are running this notebook locally, you can also download the model from the link above and place it in a `data` directory in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget --content-disposition https://surfdrive.surf.nl/files/index.php/s/XmUIlsy33vpRdCX/download\n",
    "\n",
    "! unzip GLOBALISE.word2vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the GLOBALISE pretrained model\n",
    "\n",
    "### Loading the model\n",
    "\n",
    "Execute the cell below to load the model. Please note that loading the model might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "vector_size = 100\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Corpus\n",
    "\n",
    "Below are some examples of how to use the model. You can substitute the words with any word you like, as long as it is in the vocabulary of the model/corpus. Everything needs to be in lowercase. See the Gensim documentation for more information on how to use the Word2Vec model: https://radimrehurek.com/gensim/models/word2vec.html. \n",
    "\n",
    "The first cells use the `most_similar` function from the Word2Vec model (`w2v`) to find and print the `topn` most similar words to a given word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pantchialling | pantjall | dehaij | pantch | depantjall | patchiall | pantchiall | challang | debijl | noodhulp | goudsoeker | pantsch | haaij | tapko | pantchialt | jaarvogel | depantchiall | jongedirk | buijtel | krankte | windbuijl | depantjallang | patchiallang | zuykermaalder | pantchallang | depantch | onbeschaamdh | copjagt | chialling | patchalling | boshaan | pantchiallings | salpetersoeker | overmaas | pantjalang | bonneratte | chialop | onbeschaamtheijt | pantc | patchall | patjallang | arnoldina | losboots | pantchall | desnoek | zijdeteeld | woelwater | suijkermaalder | bancq | depatchiall | kruisser | depant | debarcq | nacheribon | sorgdrager | zijdewoom | glisgis | beschutter | vantchiall | delosboot | garnaal | chailoup | beschermer | zordaan | galwet | casuaris | pandjallang | casuarus | pantj | schipio | galeij | oostendenaer | ontang | patch | burk | losboot | smapt | panthialling | bethij | breguantijn | depatch | coffijthuijn | pantsjall | contong | moesthuijn | ramsgatte | jallang | zuijerbeek | onbeschaamtheijd | pantchalling | panthiallang | pittoor | zuijkermaalder | chialoop | tanjongpour | vrctoria | vesuvius | pinxterbloem | chiloup | pantschiallang | "
     ]
    }
   ],
   "source": [
    "for i in w2v.most_similar(\"pantchialang\", topn=100):\n",
    "    print(i[0], end=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jntje | maleijer | dul | maleyer | anachoda | bappa | salim | jntie | malijer | malaijer | malim | boeginees | jurragan | parnakan | iavaan | iuragan | intie | cadier | sadulla | carim | mochamat | abdul | samat | parnackan | javaan | arabier | assan | nachoda | javaen | soedin | bouginees | mohamat | abdulla | achmat | talip | iurragan | inw | kinko | balier | zait | jnw | lim | sleman | juragan | saijit | garrang | rahim | bagus | oeij | tjina | anach | njo | jabar | boeang | tjan | mahama | karim | boeijong | aboe | jnwoonder | ganie | campar | tja | garang | balijer | troena | kamat | mallijer | anak | chin | sait | cassim | machoda | boejong | soekoer | roekoe | nio | samara | oemar | poea | lebe | hoko | miskien | vrijbalier | maijang | hoeko | salee | sech | samsoe | boegenees | naghoda | koko | gonting | tenoedin | mandarees | oesien | troeno | draman | sinko | jamal | "
     ]
    }
   ],
   "source": [
    "for i in w2v.most_similar(\"intje\", topn=100):\n",
    "    print(i[0], end=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canneel | arreecq | arreeck | cardamom | geschilden | balen | cardaman | cardamon | areecq | arreek | geschilt | overjarigen | areek | geschilde | ruijnas | bast | kurkuma | wortelen | wortel | cardemom | cannel | saffragamse | arreeq | groven | jndigo | incorrecte | ammenams | schelders | plantjes | curcuma | areeck | ougsten | affpacken | zaije | runas | schillens | moernagelen | cauwa | wortels | smakeloose | koehuijden | klenen | indigo | gekookten | zalpeter | canneer | saije | calpentijnsen | cragtelose | endeneese | canneelschilders | cheijlonsen | kannee | reuck | baelen | baalen | kanneel | pingos | sacken | varssen | anijl | ruinas | ammonams | tabacq | zaat | cauris | amm | ruias | cardanom | fijnen | cardamam | coffijbonen | cardamoin | arreck | bhaalen | zaijen | nagelen | caneell | embaleeren | bladeren | berberijen | coffijboonen | overjarige | kleenen | fordeelen | zaad | onrijpe | noten | pken | specerije | gamsen | geschild | caaneel | roggevellen | endeneesche | ingesamelden | oliteiten | peerlen | pepen | elijhanten | "
     ]
    }
   ],
   "source": [
    "for i in w2v.most_similar(\"caneel\", topn=100):\n",
    "    print(i[0], end=\" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below finds the 100 most similar words to \"Amsterdam\" that have a similarity score of 0.4 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sterdam | middelburg | amsterd | amst | zeeland | amster | amsterdm | amstm | rotterdam | delft | amsteldam | enkhuijsen | zeland | middelburgh | utrecht | ams | amsterda | amste | gravenhage | terdam | zeelant | zeiland | derwapen | enchuijsen | dam | delff | maddelburg | middelb | enckhuijsen | amstedam | enkhuijzen | aamsterdam | delfft | presidiale | enkhuisen | seeland | enckhuijzen | geredreseert | vlissingen | rdam | praesidiale | amsterdan | hage | costeux | zeelandt | wappan | hoorn | rotterdant | delburg | delf | delst | behangsels | inzeland | middelbrerg | enkhuizen | proefidiaale | praecidiale | ceulen | boodh | caamer | enckhuijs | dewees | behanghsel | amsterstam | temiddelburg | enkhuysen | zieland | alkmaar | meddelburg | cognoissemet | rotter | sdh | carode | uijtgevaren | middelburgin | kameer | delvt | leijden | zeel | praesideale | amstd | uijtregt | utregt | hoplooper | enchuy | terkamer | rabbinel | vlissinge | diale | kaner | veere | arnhem | confernee | praesidiaale | haarlem | kamier | enehuysen | siemermeer | middeburg | amstdam | "
     ]
    }
   ],
   "source": [
    "for i, p in w2v.most_similar(\"amsterdam\", topn=100):\n",
    "    if p >= 0.4:\n",
    "        print(i, end=\" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell again uses the `most_similar` function from the Word2Vec model (`w2v`), this time to find and print words similar to a given set of \"positive\" and \"negative\" terms. The vector representation of `positive` words contributes positively to the similarity computation, that of `negative` words negatively based on their vector relationships. In this example, we use this methods to find words similar to \"weder\" in the meaning of \"weather\", and not in the meaning of \"again\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weir | reegen | wint | zeewint | lugt | windt | noorde | winden | waaijende | stroom | buijen | winde | sneeuw | doorwaijen | zuijde | lucht | regenbuijen | waijende | wind | coeltjens | suijdweste | koude | vlagen | handsaem | weerligt | dewind | regenagtig | tegenstroom | doorwaijende | sonneschijn | regenen | stilte | koelte | regens | coelte | lught | hitte | stijve | lughje | zeewind | wintje | weste | warme | onstuijmig | reegenen | stroomen | koelten | zonnestraalen | delugt | warmte | handsaam | buijdige | travaden | doorbreken | inbreeken | moussom | doorwaaijende | reegende | travadig | doorstaande | doorkomende | hette | buijig | luchje | felle | afwatering | starke | kentering | overdag | stormwinden | reegens | wzw | westelijke | vloet | variable | coeltje | calte | tegenwinden | ooste | goedweer | oostelijke | noordweste | zot | waaijde | deijning | aartrijk | noordelijk | valwinden | ongestadige | doorwaaijen | slijve | suijde | caelte | lugties | firmament | regende | coeste | travodig | coelende | doorbrake | "
     ]
    }
   ],
   "source": [
    "for i in w2v.most_similar(\n",
    "    positive=[\"weder\", \"weer\", \"regen\"], negative=[\"wederom\", \"alweder\"], topn=100\n",
    "):\n",
    "    print(i[0], end=\" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ships in the Dutch East India Company (VOC) fleet were often named after places. The following cell uses the `closer_than` function to find all words in the Word2Vec model’s vocabulary whose vector representations are closer to a specified word (\"eendracht\", here meant as the name of a ship) than to another word (\"tilburg\", in this example meant as the name of a place) in terms of cosine similarity. This helps to identify words that share a stronger contextual association with \"eendracht\" (ship) compared to \"tilburg\" (place) and thus ideally filter out terms referring to places, yielding a list of potential ship names, or at least words that are more likely to be associated with ships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ende | naer | oock | noch | int | nae | schepen | retour | vant | camer | gecomen | fluijt | volck | becomen | welck | jacht | hoorn | japan | rotterdam | coninck | ditto | jagt | wint | compe | godt | lant | eijlanden | derwaerts | end | vertreck | landt | goa | geladen | stadt | tschip | bat | comende | maent | opt | chaloup | maecken | ladinge | japara | delft | oocq | gearriveert | genaemt | gemaeckt | weijnich | coningh | rhede | langh | waermede | daermede | ene | macht | ancker | originele | jnt | eijlant | nassauw | augustij | vrede | quartieren | wapen | voijagie | cattij | middagh | achter | opde | vaderlant | portugees | geseijde | leeuw | dirck | cargasoen | verwachten | mauritius | rijck | chialoep | dach | namentlijck | eijlandt | geladene | vlissingen | jachten | battavia | gelyck | seecker | wingurla | gescheept | amst | portugesen | iapan | comste | stondt | nederlants | arent | nacht | vercocht\n"
     ]
    }
   ],
   "source": [
    "words = w2v.closer_than(\"eendracht\", \"tilburg\")\n",
    "\n",
    "output = \" | \".join(words[:100])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogy generation can provide insights about historical semantics and how certain terms relate to one another in specific domains. To do this, you can use the `most_similar` with a combination of \"positive\" and \"negative\" word vectors. For example, running the following cell yields the ten best fitting words (based on their vector respresentation) for the analogy \"noten is to banda as [X] is to ceylon\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'noten' is to 'banda' as the following are to 'ceylon':\n",
      "cardamon (similarity: 0.5871)\n",
      "baalen (similarity: 0.5764)\n",
      "ruijnas (similarity: 0.5662)\n",
      "baaltjes (similarity: 0.5633)\n",
      "kardamom (similarity: 0.5629)\n",
      "chiancossen (similarity: 0.5529)\n",
      "cardamam (similarity: 0.5451)\n",
      "wortelen (similarity: 0.5399)\n",
      "caneel (similarity: 0.5394)\n",
      "cardaman (similarity: 0.5378)\n"
     ]
    }
   ],
   "source": [
    "results = w2v.most_similar(positive=[\"noten\", \"ceylon\"], negative=[\"banda\"], topn=10)\n",
    "\n",
    "print(\"'noten' is to 'banda' as the following are to 'ceylon':\")\n",
    "for word, similarity in results:\n",
    "    print(f\"{word} (similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GLOBALISE Word2Vec model\n",
    "\n",
    "The cells below show how we trained our model. Following the methodology allows you to easily retrain it (e.g. with different parameters or on a subset of the corpus).\n",
    "\n",
    "### Loading the Libraries and Configuring Preprocessing and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnAKM6s7mNiF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.corpora.textcorpus import TextDirectoryCorpus\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    remove_stopword_tokens,\n",
    "    remove_short_tokens,\n",
    "    lower_to_unicode,\n",
    "    strip_multiple_whitespaces,\n",
    ")\n",
    "from gensim.utils import deaccent, simple_tokenize, effective_n_jobs\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Setting\n",
    "vector_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn1F7VCtm75T"
   },
   "source": [
    "### Downloading the Data\n",
    "\n",
    "The data can be downloaded from the GLOBALISE Dataverse: https://datasets.iisg.amsterdam/dataverse/globalise. For this experiment, we’re working with version v2.0 of the transcriptions dataset:\n",
    "\n",
    "- GLOBALISE project, 2024, \"VOC transcriptions v2 - GLOBALISE\", https://hdl.handle.net/10622/LVXSBW, IISH Data Collection\n",
    "\n",
    "The project conveniently provides a file with pointers to all txt files in this dataset that we can download automatically. We are using `wget` to download the files. First the file with pointers, which we will use to download all txt files. This can take a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WwZv4DGm89A",
    "outputId": "52f5dbdd-7e93-4f0f-8a1d-1e6a9ee53b81"
   },
   "outputs": [],
   "source": [
    "! mkdir -p data && wget https://datasets.iisg.amsterdam/api/access/datafile/33172?gbrecs=true -O data/globalise_transcriptions_v2_txt.tab --content-disposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/txt && wget -i data/globalise_transcriptions_v2_txt.tab -P data/txt/ --content-disposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRX5hAEWnA1K",
    "outputId": "c6fb4276-d89b-4039-b79b-e2d485a42df8"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOuHXoGonMhF"
   },
   "source": [
    "We now have a collection of text files, in which each file represents the text per inventory number.\n",
    "\n",
    "The files need a bit of pre-processing before we can work with it. What needs to be done:\n",
    "\n",
    "- Remove all lines starting with `#+ `. These are comments and not part of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sh13_NrUolCu"
   },
   "outputs": [],
   "source": [
    "def preprocess_txt(file_path):\n",
    "    print(\"Processing\", file_path)\n",
    "\n",
    "    # Open the textfile\n",
    "    with open(file_path) as infile:\n",
    "        text = infile.read()\n",
    "\n",
    "    lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line.startswith(\"#+ \"):\n",
    "            continue\n",
    "        else:\n",
    "            lines.append(line)\n",
    "\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    # Save the cleaned version\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        outfile.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLbIzqi30DDM",
    "outputId": "ed1d58f8-b3fe-4a1c-fb7c-f8cc9e7763f0"
   },
   "outputs": [],
   "source": [
    "FOLDER = \"data/txt\"\n",
    "\n",
    "for f in os.listdir(FOLDER):\n",
    "    filepath = os.path.join(FOLDER, f)\n",
    "    preprocess_txt(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raHVnxe30tv0"
   },
   "source": [
    "### Processing\n",
    "\n",
    "Now that we have the data in a usable format, we can start processing it. We will use the [Gensim](https://radimrehurek.com/gensim/) library to train a Word2Vec model on the text data. For this, we first create a Corpus object that will be used to feed text to the model. We use a custom implementation of the `gensim.corpora.textcorpus.TextCorpus` class to now have a cutoff for the number of words in the vocabulary (standard settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A-K78cd0TKs"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CustomTextDirectoryCorpus(TextDirectoryCorpus):\n",
    "    \"\"\"\n",
    "    Custom class to set the `prune_at` gensim.Dictionary parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input,\n",
    "        dictionary=None,\n",
    "        metadata=False,\n",
    "        character_filters=None,\n",
    "        tokenizer=None,\n",
    "        token_filters=None,\n",
    "        min_depth=0,\n",
    "        max_depth=None,\n",
    "        pattern=None,\n",
    "        exclude_pattern=None,\n",
    "        lines_are_documents=False,\n",
    "        encoding=\"utf-8\",\n",
    "        dictionary_prune_at=2_000_000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._min_depth = min_depth\n",
    "        self._max_depth = sys.maxsize if max_depth is None else max_depth\n",
    "        self.pattern = pattern\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.lines_are_documents = lines_are_documents\n",
    "        self.encoding = encoding\n",
    "\n",
    "        self.dictionary_prune_at = dictionary_prune_at\n",
    "\n",
    "        self.input = input\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.character_filters = character_filters\n",
    "        if self.character_filters is None:\n",
    "            self.character_filters = [\n",
    "                lower_to_unicode,\n",
    "                deaccent,\n",
    "                strip_multiple_whitespaces,\n",
    "            ]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = simple_tokenize\n",
    "\n",
    "        self.token_filters = token_filters\n",
    "        if self.token_filters is None:\n",
    "            self.token_filters = [remove_short_tokens, remove_stopword_tokens]\n",
    "\n",
    "        self.length = None\n",
    "        self.dictionary = None\n",
    "        self.init_dictionary(dictionary)\n",
    "\n",
    "        super(CustomTextDirectoryCorpus, self).__init__(\n",
    "            input, self.dictionary, metadata, **kwargs\n",
    "        )\n",
    "\n",
    "    def init_dictionary(self, dictionary):\n",
    "        \"\"\"Initialize/update dictionary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
    "            If a dictionary is provided, it will not be updated with the given corpus on initialization.\n",
    "            If None - new dictionary will be built for the given corpus.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        If self.input is None - make nothing.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.dictionary = dictionary if dictionary is not None else Dictionary()\n",
    "\n",
    "        if self.input is not None:\n",
    "            if dictionary is None:\n",
    "                logger.info(\"Initializing dictionary\")\n",
    "                metadata_setting = self.metadata\n",
    "                self.metadata = False\n",
    "                self.dictionary.add_documents(\n",
    "                    self.get_texts(), prune_at=self.dictionary_prune_at\n",
    "                )\n",
    "                self.metadata = metadata_setting\n",
    "            else:\n",
    "                logger.info(\"Input stream provided but dictionary already initialized\")\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"No input document stream provided; assuming dictionary will be initialized some other way.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKsImCrO04I7"
   },
   "outputs": [],
   "source": [
    "class SentencesIterator:\n",
    "    def __init__(self, generator_function):\n",
    "        self.generator_function = generator_function\n",
    "        self.generator = self.generator_function()\n",
    "\n",
    "    def __iter__(self):\n",
    "        # reset the generator\n",
    "        self.generator = self.generator_function()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        result = next(self.generator)\n",
    "        if result is None:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSsw2ZZReOQ6"
   },
   "source": [
    "With the above code we can generate our own `corpus’ object with a slightly bigger dictionary size than in Gensim’s standard library. We set it to 20M, since we are also interested in the less frequently occurring words (e.g. spelling varieties). We can filter later on minimum frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FejMGsr0y2s",
    "outputId": "393df7d2-0d65-41d0-e516-a6fc693c353d"
   },
   "outputs": [],
   "source": [
    "corpus = CustomTextDirectoryCorpus(FOLDER, dictionary_prune_at=20_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s save the corpus object to disk, so we can use it later on and don’t have to re-run the pre-processing steps. Comment and uncomment the respective code below to run the pre-processing steps or load the corpus object from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/corpus.pkl\", \"rb\") as f:\n",
    "#     corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the Word2Vec model. For this, we need to feed it the corpus object multiple times. We do so by initializing an iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OZBxPtwO01O7"
   },
   "outputs": [],
   "source": [
    "texts = SentencesIterator(corpus.get_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpaId718f2SY"
   },
   "source": [
    "Now, let’s create a Word2Vec embedding. You can set the number of workers to your CPU count (minus 1). Again, this can take a while.\n",
    "\n",
    "You can experiment with the parameters of the Word2Vec model, such as the vector size, window size, and minimum frequency, but this can lead to a bigger model, longer training time, and not necessarily better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgOIaxeN1i8q"
   },
   "outputs": [],
   "source": [
    "workers = effective_n_jobs(max(os.cpu_count() - 1, 1))\n",
    "w2v = Word2Vec(\n",
    "    texts, vector_size=vector_size, window=5, min_count=5, workers=workers, epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgL6DmA2Lnv9"
   },
   "source": [
    "Now, let’s save the embedding for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yarIb2np1JgS"
   },
   "outputs": [],
   "source": [
    "w2v.wv.save_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
