{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#home","title":"Home","text":"<p>The aim of the GLOBALISE project is to develop an online infrastructure that unlocks the key series of VOC documents and reports for advanced research methods. On this site, we share experiments and prototypes related to our datasets and infrastructure. We welcome your feedback.</p>"},{"location":"#experiments","title":"Experiments","text":"<ul> <li>Word Embeddings trained on the c. 5 million pages of VOC transcriptions, enabling the identification of spelling variants, synonyms, and other semantic relationships for a given word.</li> <li>Viewer for transcriptions of the c. 5 million pages of VOC documents that comprise the GLOBALISE corpus.</li> <li>Visualization of places occurring in the c. 5 million pages of VOC documents that comprise the GLOBALISE corpus.</li> <li>Search Interface for the General Missives of the VOC, as edited and published in 14 book volumes over the period 1960-2017.</li> <li>Text-Fabric Serialization of the General Missives of the VOC, especially suited for computational analysis of this corpus.</li> </ul>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/","title":"Word2Vec Model","text":"<p>Date: July 2024 Status: Ongoing experiment People involved: Leon van Wissen</p> In\u00a0[\u00a0]: Copied! <pre>! wget --content-disposition https://surfdrive.surf.nl/files/index.php/s/XmUIlsy33vpRdCX/download\n\n! unzip GLOBALISE.word2vec.zip\n</pre> ! wget --content-disposition https://surfdrive.surf.nl/files/index.php/s/XmUIlsy33vpRdCX/download  ! unzip GLOBALISE.word2vec.zip In\u00a0[\u00a0]: Copied! <pre>! pip install gensim -U\n</pre> ! pip install gensim -U In\u00a0[1]: Copied! <pre>import os\nimport sys\nimport logging\nimport pickle\n\nfrom gensim.models import Word2Vec, KeyedVectors\n\nvector_size = 100\n\nw2v = KeyedVectors.load_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\")\n</pre> import os import sys import logging import pickle  from gensim.models import Word2Vec, KeyedVectors  vector_size = 100  w2v = KeyedVectors.load_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\") In\u00a0[2]: Copied! <pre>for i in w2v.most_similar(\"pantchialang\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"pantchialang\", topn=100):     print(i[0], end=\" | \") <pre>pantchialling | pantjall | dehaij | pantch | depantjall | patchiall | pantchiall | challang | debijl | noodhulp | goudsoeker | pantsch | haaij | tapko | pantchialt | jaarvogel | depantchiall | jongedirk | buijtel | krankte | windbuijl | depantjallang | patchiallang | zuykermaalder | pantchallang | depantch | onbeschaamdh | copjagt | chialling | patchalling | boshaan | pantchiallings | salpetersoeker | overmaas | pantjalang | bonneratte | chialop | onbeschaamtheijt | pantc | patchall | patjallang | arnoldina | losboots | pantchall | desnoek | zijdeteeld | woelwater | suijkermaalder | bancq | depatchiall | kruisser | depant | debarcq | nacheribon | sorgdrager | zijdewoom | glisgis | beschutter | vantchiall | delosboot | garnaal | chailoup | beschermer | zordaan | galwet | casuaris | pandjallang | casuarus | pantj | schipio | galeij | oostendenaer | ontang | patch | burk | losboot | smapt | panthialling | bethij | breguantijn | depatch | coffijthuijn | pantsjall | contong | moesthuijn | ramsgatte | jallang | zuijerbeek | onbeschaamtheijd | pantchalling | panthiallang | pittoor | zuijkermaalder | chialoop | tanjongpour | vrctoria | vesuvius | pinxterbloem | chiloup | pantschiallang | </pre> In\u00a0[3]: Copied! <pre>for i in w2v.most_similar(\"intje\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"intje\", topn=100):     print(i[0], end=\" | \") <pre>jntje | maleijer | dul | maleyer | anachoda | bappa | salim | jntie | malijer | malaijer | malim | boeginees | jurragan | parnakan | iavaan | iuragan | intie | cadier | sadulla | carim | mochamat | abdul | samat | parnackan | javaan | arabier | assan | nachoda | javaen | soedin | bouginees | mohamat | abdulla | achmat | talip | iurragan | inw | kinko | balier | zait | jnw | lim | sleman | juragan | saijit | garrang | rahim | bagus | oeij | tjina | anach | njo | jabar | boeang | tjan | mahama | karim | boeijong | aboe | jnwoonder | ganie | campar | tja | garang | balijer | troena | kamat | mallijer | anak | chin | sait | cassim | machoda | boejong | soekoer | roekoe | nio | samara | oemar | poea | lebe | hoko | miskien | vrijbalier | maijang | hoeko | salee | sech | samsoe | boegenees | naghoda | koko | gonting | tenoedin | mandarees | oesien | troeno | draman | sinko | jamal | </pre> In\u00a0[4]: Copied! <pre>for i in w2v.most_similar(\"caneel\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"caneel\", topn=100):     print(i[0], end=\" | \") <pre>canneel | arreecq | arreeck | cardamom | geschilden | balen | cardaman | cardamon | areecq | arreek | geschilt | overjarigen | areek | geschilde | ruijnas | bast | kurkuma | wortelen | wortel | cardemom | cannel | saffragamse | arreeq | groven | jndigo | incorrecte | ammenams | schelders | plantjes | curcuma | areeck | ougsten | affpacken | zaije | runas | schillens | moernagelen | cauwa | wortels | smakeloose | koehuijden | klenen | indigo | gekookten | zalpeter | canneer | saije | calpentijnsen | cragtelose | endeneese | canneelschilders | cheijlonsen | kannee | reuck | baelen | baalen | kanneel | pingos | sacken | varssen | anijl | ruinas | ammonams | tabacq | zaat | cauris | amm | ruias | cardanom | fijnen | cardamam | coffijbonen | cardamoin | arreck | bhaalen | zaijen | nagelen | caneell | embaleeren | bladeren | berberijen | coffijboonen | overjarige | kleenen | fordeelen | zaad | onrijpe | noten | pken | specerije | gamsen | geschild | caaneel | roggevellen | endeneesche | ingesamelden | oliteiten | peerlen | pepen | elijhanten | </pre> <p>The cell below finds the 100 most similar words to \"Amsterdam\" that have a similarity score of 0.4 or higher.</p> In\u00a0[5]: Copied! <pre>for i, p in w2v.most_similar(\"amsterdam\", topn=100):\n    if p &gt;= 0.4:\n        print(i, end=\" | \")\n</pre> for i, p in w2v.most_similar(\"amsterdam\", topn=100):     if p &gt;= 0.4:         print(i, end=\" | \") <pre>sterdam | middelburg | amsterd | amst | zeeland | amster | amsterdm | amstm | rotterdam | delft | amsteldam | enkhuijsen | zeland | middelburgh | utrecht | ams | amsterda | amste | gravenhage | terdam | zeelant | zeiland | derwapen | enchuijsen | dam | delff | maddelburg | middelb | enckhuijsen | amstedam | enkhuijzen | aamsterdam | delfft | presidiale | enkhuisen | seeland | enckhuijzen | geredreseert | vlissingen | rdam | praesidiale | amsterdan | hage | costeux | zeelandt | wappan | hoorn | rotterdant | delburg | delf | delst | behangsels | inzeland | middelbrerg | enkhuizen | proefidiaale | praecidiale | ceulen | boodh | caamer | enckhuijs | dewees | behanghsel | amsterstam | temiddelburg | enkhuysen | zieland | alkmaar | meddelburg | cognoissemet | rotter | sdh | carode | uijtgevaren | middelburgin | kameer | delvt | leijden | zeel | praesideale | amstd | uijtregt | utregt | hoplooper | enchuy | terkamer | rabbinel | vlissinge | diale | kaner | veere | arnhem | confernee | praesidiaale | haarlem | kamier | enehuysen | siemermeer | middeburg | amstdam | </pre> <p>The following cell again uses the <code>most_similar</code> function from the Word2Vec model (<code>w2v</code>), this time to find and print words similar to a given set of \"positive\" and \"negative\" terms. The vector representation of <code>positive</code> words contributes positively to the similarity computation, that of <code>negative</code> words negatively based on their vector relationships. In this example, we use this methods to find words similar to \"weder\" in the meaning of \"weather\", and not in the meaning of \"again\".</p> In\u00a0[6]: Copied! <pre>for i in w2v.most_similar(\n    positive=[\"weder\", \"weer\", \"regen\"], negative=[\"wederom\", \"alweder\"], topn=100\n):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(     positive=[\"weder\", \"weer\", \"regen\"], negative=[\"wederom\", \"alweder\"], topn=100 ):     print(i[0], end=\" | \") <pre>weir | reegen | wint | zeewint | lugt | windt | noorde | winden | waaijende | stroom | buijen | winde | sneeuw | doorwaijen | zuijde | lucht | regenbuijen | waijende | wind | coeltjens | suijdweste | koude | vlagen | handsaem | weerligt | dewind | regenagtig | tegenstroom | doorwaijende | sonneschijn | regenen | stilte | koelte | regens | coelte | lught | hitte | stijve | lughje | zeewind | wintje | weste | warme | onstuijmig | reegenen | stroomen | koelten | zonnestraalen | delugt | warmte | handsaam | buijdige | travaden | doorbreken | inbreeken | moussom | doorwaaijende | reegende | travadig | doorstaande | doorkomende | hette | buijig | luchje | felle | afwatering | starke | kentering | overdag | stormwinden | reegens | wzw | westelijke | vloet | variable | coeltje | calte | tegenwinden | ooste | goedweer | oostelijke | noordweste | zot | waaijde | deijning | aartrijk | noordelijk | valwinden | ongestadige | doorwaaijen | slijve | suijde | caelte | lugties | firmament | regende | coeste | travodig | coelende | doorbrake | </pre> <p>Ships in the Dutch East India Company (VOC) fleet were often named after places. The following cell uses the <code>closer_than</code> function to find all words in the Word2Vec model\u2019s vocabulary whose vector representations are closer to a specified word (\"eendracht\", here meant as the name of a ship) than to another word (\"tilburg\", in this example meant as the name of a place) in terms of cosine similarity. This helps to identify words that share a stronger contextual association with \"eendracht\" (ship) compared to \"tilburg\" (place) and thus ideally filter out terms referring to places, yielding a list of potential ship names, or at least words that are more likely to be associated with ships.</p> In\u00a0[7]: Copied! <pre>words = w2v.closer_than(\"eendracht\", \"tilburg\")\n\noutput = \" | \".join(words[:100])\n\nprint(output)\n</pre> words = w2v.closer_than(\"eendracht\", \"tilburg\")  output = \" | \".join(words[:100])  print(output) <pre>ende | naer | oock | noch | int | nae | schepen | retour | vant | camer | gecomen | fluijt | volck | becomen | welck | jacht | hoorn | japan | rotterdam | coninck | ditto | jagt | wint | compe | godt | lant | eijlanden | derwaerts | end | vertreck | landt | goa | geladen | stadt | tschip | bat | comende | maent | opt | chaloup | maecken | ladinge | japara | delft | oocq | gearriveert | genaemt | gemaeckt | weijnich | coningh | rhede | langh | waermede | daermede | ene | macht | ancker | originele | jnt | eijlant | nassauw | augustij | vrede | quartieren | wapen | voijagie | cattij | middagh | achter | opde | vaderlant | portugees | geseijde | leeuw | dirck | cargasoen | verwachten | mauritius | rijck | chialoep | dach | namentlijck | eijlandt | geladene | vlissingen | jachten | battavia | gelyck | seecker | wingurla | gescheept | amst | portugesen | iapan | comste | stondt | nederlants | arent | nacht | vercocht\n</pre> <p>Analogy generation can provide insights about historical semantics and how certain terms relate to one another in specific domains. To do this, you can use the <code>most_similar</code> with a combination of \"positive\" and \"negative\" word vectors. For example, running the following cell yields the ten best fitting words (based on their vector respresentation) for the analogy \"noten is to banda as [X] is to ceylon\".</p> In\u00a0[8]: Copied! <pre>results = w2v.most_similar(positive=[\"noten\", \"ceylon\"], negative=[\"banda\"], topn=10)\n\nprint(\"'noten' is to 'banda' as the following are to 'ceylon':\")\nfor word, similarity in results:\n    print(f\"{word} (similarity: {similarity:.4f})\")\n</pre> results = w2v.most_similar(positive=[\"noten\", \"ceylon\"], negative=[\"banda\"], topn=10)  print(\"'noten' is to 'banda' as the following are to 'ceylon':\") for word, similarity in results:     print(f\"{word} (similarity: {similarity:.4f})\") <pre>'noten' is to 'banda' as the following are to 'ceylon':\ncardamon (similarity: 0.5871)\nbaalen (similarity: 0.5764)\nruijnas (similarity: 0.5662)\nbaaltjes (similarity: 0.5633)\nkardamom (similarity: 0.5629)\nchiancossen (similarity: 0.5529)\ncardamam (similarity: 0.5451)\nwortelen (similarity: 0.5399)\ncaneel (similarity: 0.5394)\ncardaman (similarity: 0.5378)\n</pre> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nimport logging\nimport pickle\n\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.corpora.textcorpus import TextDirectoryCorpus\n\nfrom gensim.corpora.dictionary import Dictionary\n\nfrom gensim.parsing.preprocessing import (\n    remove_stopword_tokens,\n    remove_short_tokens,\n    lower_to_unicode,\n    strip_multiple_whitespaces,\n)\nfrom gensim.utils import deaccent, simple_tokenize, effective_n_jobs\n\nlogging.basicConfig(\n    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n)\nlogging.getLogger().setLevel(logging.INFO)\n\n# Setting\nvector_size = 100\n</pre> import os import sys import logging import pickle  from gensim.models import Word2Vec, KeyedVectors from gensim.corpora.textcorpus import TextDirectoryCorpus  from gensim.corpora.dictionary import Dictionary  from gensim.parsing.preprocessing import (     remove_stopword_tokens,     remove_short_tokens,     lower_to_unicode,     strip_multiple_whitespaces, ) from gensim.utils import deaccent, simple_tokenize, effective_n_jobs  logging.basicConfig(     format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO ) logging.getLogger().setLevel(logging.INFO)  # Setting vector_size = 100 In\u00a0[\u00a0]: Copied! <pre>! mkdir -p data &amp;&amp; wget https://datasets.iisg.amsterdam/api/access/datafile/33172?gbrecs=true -O data/globalise_transcriptions_v2_txt.tab --content-disposition\n</pre> ! mkdir -p data &amp;&amp; wget https://datasets.iisg.amsterdam/api/access/datafile/33172?gbrecs=true -O data/globalise_transcriptions_v2_txt.tab --content-disposition In\u00a0[\u00a0]: Copied! <pre>! mkdir -p data/txt &amp;&amp; wget -i data/globalise_transcriptions_v2_txt.tab -P data/txt/ --content-disposition\n</pre> ! mkdir -p data/txt &amp;&amp; wget -i data/globalise_transcriptions_v2_txt.tab -P data/txt/ --content-disposition <p>We now have a collection of text files, in which each file represents the text per inventory number.</p> <p>The files need a bit of pre-processing before we can work with it. What needs to be done:</p> <ul> <li>Remove all lines starting with <code>#+ </code>. These are comments and not part of the text.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def preprocess_txt(file_path):\n    print(\"Processing\", file_path)\n\n    # Open the textfile\n    with open(file_path) as infile:\n        text = infile.read()\n\n    lines = []\n    for line in text.split(\"\\n\"):\n        if line.startswith(\"#+ \"):\n            continue\n        else:\n            lines.append(line)\n\n    text = \"\\n\".join(lines)\n\n    # Save the cleaned version\n    with open(file_path, \"w\") as outfile:\n        outfile.write(text)\n</pre> def preprocess_txt(file_path):     print(\"Processing\", file_path)      # Open the textfile     with open(file_path) as infile:         text = infile.read()      lines = []     for line in text.split(\"\\n\"):         if line.startswith(\"#+ \"):             continue         else:             lines.append(line)      text = \"\\n\".join(lines)      # Save the cleaned version     with open(file_path, \"w\") as outfile:         outfile.write(text) In\u00a0[\u00a0]: Copied! <pre>FOLDER = \"data/txt\"\n\nfor f in os.listdir(FOLDER):\n    filepath = os.path.join(FOLDER, f)\n    preprocess_txt(filepath)\n</pre> FOLDER = \"data/txt\"  for f in os.listdir(FOLDER):     filepath = os.path.join(FOLDER, f)     preprocess_txt(filepath) In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\n\n\nclass CustomTextDirectoryCorpus(TextDirectoryCorpus):\n    \"\"\"\n    Custom class to set the `prune_at` gensim.Dictionary parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        input,\n        dictionary=None,\n        metadata=False,\n        character_filters=None,\n        tokenizer=None,\n        token_filters=None,\n        min_depth=0,\n        max_depth=None,\n        pattern=None,\n        exclude_pattern=None,\n        lines_are_documents=False,\n        encoding=\"utf-8\",\n        dictionary_prune_at=2_000_000,\n        **kwargs,\n    ):\n        self._min_depth = min_depth\n        self._max_depth = sys.maxsize if max_depth is None else max_depth\n        self.pattern = pattern\n        self.exclude_pattern = exclude_pattern\n        self.lines_are_documents = lines_are_documents\n        self.encoding = encoding\n\n        self.dictionary_prune_at = dictionary_prune_at\n\n        self.input = input\n        self.metadata = metadata\n\n        self.character_filters = character_filters\n        if self.character_filters is None:\n            self.character_filters = [\n                lower_to_unicode,\n                deaccent,\n                strip_multiple_whitespaces,\n            ]\n\n        self.tokenizer = tokenizer\n        if self.tokenizer is None:\n            self.tokenizer = simple_tokenize\n\n        self.token_filters = token_filters\n        if self.token_filters is None:\n            self.token_filters = [remove_short_tokens, remove_stopword_tokens]\n\n        self.length = None\n        self.dictionary = None\n        self.init_dictionary(dictionary)\n\n        super(CustomTextDirectoryCorpus, self).__init__(\n            input, self.dictionary, metadata, **kwargs\n        )\n\n    def init_dictionary(self, dictionary):\n        \"\"\"Initialize/update dictionary.\n\n        Parameters\n        ----------\n        dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n            If a dictionary is provided, it will not be updated with the given corpus on initialization.\n            If None - new dictionary will be built for the given corpus.\n\n        Notes\n        -----\n        If self.input is None - make nothing.\n\n        \"\"\"\n\n        self.dictionary = dictionary if dictionary is not None else Dictionary()\n\n        if self.input is not None:\n            if dictionary is None:\n                logger.info(\"Initializing dictionary\")\n                metadata_setting = self.metadata\n                self.metadata = False\n                self.dictionary.add_documents(\n                    self.get_texts(), prune_at=self.dictionary_prune_at\n                )\n                self.metadata = metadata_setting\n            else:\n                logger.info(\"Input stream provided but dictionary already initialized\")\n        else:\n            logger.warning(\n                \"No input document stream provided; assuming dictionary will be initialized some other way.\"\n            )\n</pre> logger = logging.getLogger(__name__)   class CustomTextDirectoryCorpus(TextDirectoryCorpus):     \"\"\"     Custom class to set the `prune_at` gensim.Dictionary parameter.     \"\"\"      def __init__(         self,         input,         dictionary=None,         metadata=False,         character_filters=None,         tokenizer=None,         token_filters=None,         min_depth=0,         max_depth=None,         pattern=None,         exclude_pattern=None,         lines_are_documents=False,         encoding=\"utf-8\",         dictionary_prune_at=2_000_000,         **kwargs,     ):         self._min_depth = min_depth         self._max_depth = sys.maxsize if max_depth is None else max_depth         self.pattern = pattern         self.exclude_pattern = exclude_pattern         self.lines_are_documents = lines_are_documents         self.encoding = encoding          self.dictionary_prune_at = dictionary_prune_at          self.input = input         self.metadata = metadata          self.character_filters = character_filters         if self.character_filters is None:             self.character_filters = [                 lower_to_unicode,                 deaccent,                 strip_multiple_whitespaces,             ]          self.tokenizer = tokenizer         if self.tokenizer is None:             self.tokenizer = simple_tokenize          self.token_filters = token_filters         if self.token_filters is None:             self.token_filters = [remove_short_tokens, remove_stopword_tokens]          self.length = None         self.dictionary = None         self.init_dictionary(dictionary)          super(CustomTextDirectoryCorpus, self).__init__(             input, self.dictionary, metadata, **kwargs         )      def init_dictionary(self, dictionary):         \"\"\"Initialize/update dictionary.          Parameters         ----------         dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional             If a dictionary is provided, it will not be updated with the given corpus on initialization.             If None - new dictionary will be built for the given corpus.          Notes         -----         If self.input is None - make nothing.          \"\"\"          self.dictionary = dictionary if dictionary is not None else Dictionary()          if self.input is not None:             if dictionary is None:                 logger.info(\"Initializing dictionary\")                 metadata_setting = self.metadata                 self.metadata = False                 self.dictionary.add_documents(                     self.get_texts(), prune_at=self.dictionary_prune_at                 )                 self.metadata = metadata_setting             else:                 logger.info(\"Input stream provided but dictionary already initialized\")         else:             logger.warning(                 \"No input document stream provided; assuming dictionary will be initialized some other way.\"             ) In\u00a0[\u00a0]: Copied! <pre>class SentencesIterator:\n    def __init__(self, generator_function):\n        self.generator_function = generator_function\n        self.generator = self.generator_function()\n\n    def __iter__(self):\n        # reset the generator\n        self.generator = self.generator_function()\n        return self\n\n    def __next__(self):\n        result = next(self.generator)\n        if result is None:\n            raise StopIteration\n        else:\n            return result\n</pre> class SentencesIterator:     def __init__(self, generator_function):         self.generator_function = generator_function         self.generator = self.generator_function()      def __iter__(self):         # reset the generator         self.generator = self.generator_function()         return self      def __next__(self):         result = next(self.generator)         if result is None:             raise StopIteration         else:             return result <p>With the above code we can generate our own `corpus\u2019 object with a slightly bigger dictionary size than in Gensim\u2019s standard library. We set it to 20M, since we are also interested in the less frequently occurring words (e.g. spelling varieties). We can filter later on minimum frequency.</p> In\u00a0[\u00a0]: Copied! <pre>corpus = CustomTextDirectoryCorpus(FOLDER, dictionary_prune_at=20_000_000)\n</pre> corpus = CustomTextDirectoryCorpus(FOLDER, dictionary_prune_at=20_000_000) <p>Now let\u2019s save the corpus object to disk, so we can use it later on and don\u2019t have to re-run the pre-processing steps. Comment and uncomment the respective code below to run the pre-processing steps or load the corpus object from disk.</p> In\u00a0[\u00a0]: Copied! <pre>with open(\"data/corpus.pkl\", \"wb\") as f:\n    pickle.dump(corpus, f)\n</pre> with open(\"data/corpus.pkl\", \"wb\") as f:     pickle.dump(corpus, f) In\u00a0[\u00a0]: Copied! <pre># with open(\"data/corpus.pkl\", \"rb\") as f:\n#     corpus = pickle.load(f)\n</pre> # with open(\"data/corpus.pkl\", \"rb\") as f: #     corpus = pickle.load(f) <p>The next step is to train the Word2Vec model. For this, we need to feed it the corpus object multiple times. We do so by initializing an iterator:</p> In\u00a0[\u00a0]: Copied! <pre>texts = SentencesIterator(corpus.get_texts)\n</pre> texts = SentencesIterator(corpus.get_texts) <p>Now, let\u2019s create a Word2Vec embedding. You can set the number of workers to your CPU count (minus 1). Again, this can take a while.</p> <p>You can experiment with the parameters of the Word2Vec model, such as the vector size, window size, and minimum frequency, but this can lead to a bigger model, longer training time, and not necessarily better results.</p> In\u00a0[\u00a0]: Copied! <pre>workers = effective_n_jobs(max(os.cpu_count() - 1, 1))\nw2v = Word2Vec(\n    texts, vector_size=vector_size, window=5, min_count=5, workers=workers, epochs=5\n)\n</pre> workers = effective_n_jobs(max(os.cpu_count() - 1, 1)) w2v = Word2Vec(     texts, vector_size=vector_size, window=5, min_count=5, workers=workers, epochs=5 ) <p>Now, let\u2019s save the embedding for future use.</p> In\u00a0[\u00a0]: Copied! <pre>w2v.wv.save_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\")\n</pre> w2v.wv.save_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\")"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#word2vec-model","title":"Word2Vec Model\u00b6","text":""},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#introduction","title":"Introduction\u00b6","text":"<p>We trained a Word2Vec model on the GLOBALISE Transcriptions, creating vector representations of words based on their context. By leveraging this model you can:</p> <ol> <li>Find Spelling Variants and Synonyms: Discover alternative spellings or synonyms of a word by identifying those with similar vector representations. This is particularly useful for early modern texts with inconsistent orthography.</li> <li>Contextual Similarity: Locate words that frequently appear in similar contexts, shedding light on semantic relationships. For instance, the term <code>plantage</code> (plantation) might reveal associations with specific crops or geographic regions.</li> <li>Advanced Semantic Queries: Perform tasks such as analogy generation (e.g., <code>noten is to banda as [X] is to ceylon</code>) and compute word similarities. These functionalities help researchers uncover patterns and insights from the corpus that are difficult to detect manually.</li> </ol> <p>This notebook guides you through loading and running our pretrained model and provides some examples of queries.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#short-user-guide","title":"Short User Guide\u00b6","text":"<p>Option 1 (Google Colab):</p> <p></p> <ol> <li>Open this notebook in Google Colab by clicking the badge above.</li> <li>Run the cells below to load the model and start querying the corpus.</li> </ol> <p>Option 2 (local):</p> <p>Download this notebook and our pretrained model, follow the cells below, and start exploring the corpus.</p> <p>Download links:</p> <ul> <li>Notebook: https://github.com/globalise-huygens/lab.globalise.huygens.knaw.nl/blob/main/docs/experiments/GLOBALISE_Word2Vec_Lab.ipynb</li> <li>Pretrained model (100 dimensions, 645MB): https://surfdrive.surf.nl/files/index.php/s/XmUIlsy33vpRdCX Note: download and unzip the <code>GLOBALISE.word2vec.zip</code> file in a <code>data</code> directory in the same folder as this notebook for the cells below to work. The cells below will do this for you if you have the <code>wget</code> and <code>unzip</code> commands available.</li> </ul> <p>If you haven\u2019t used Jupyter notebooks before, we recommend looking up a user guide online. Anaconda is an easy-to-use package. Make sure you have the required libraries (such as Gensim) installed in the Python environment you\u2019re using.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#download-the-pretrained-model-and-install-gensim","title":"Download the Pretrained Model and install Gensim\u00b6","text":"<p>The cell below downloads the pretrained model and installs the Gensim library. If you are running this notebook locally, you can also download the model from the link above and place it in a <code>data</code> directory in the same folder as this notebook.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#running-the-globalise-pretrained-model","title":"Running the GLOBALISE pretrained model\u00b6","text":""},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#loading-the-model","title":"Loading the model\u00b6","text":"<p>Execute the cell below to load the model.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#analyzing-the-corpus","title":"Analyzing the Corpus\u00b6","text":"<p>Below are some examples of how to use the model. You can substitute the words with any word you like, as long as it is in the vocabulary of the model/corpus. Everything needs to be in lowercase. See the Gensim documentation for more information on how to use the Word2Vec model: https://radimrehurek.com/gensim/models/word2vec.html.</p> <p>The first cells use the <code>most_similar</code> function from the Word2Vec model (<code>w2v</code>) to find and print the <code>topn</code> most similar words to a given word.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#training-the-globalise-word2vec-model","title":"Training the GLOBALISE Word2Vec model\u00b6","text":"<p>The cells below show how we trained our model. Following the methodology allows you to easily retrain it (e.g. with different parameters or on a subset of the corpus).</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#loading-the-libraries-and-configuring-preprocessing-and-logging","title":"Loading the Libraries and Configuring Preprocessing and Logging\u00b6","text":""},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#downloading-the-data","title":"Downloading the Data\u00b6","text":"<p>The data can be downloaded from the GLOBALISE Dataverse: https://datasets.iisg.amsterdam/dataverse/globalise. For this experiment, we\u2019re working with version v2.0 of the transcriptions dataset:</p> <ul> <li>GLOBALISE project, 2024, \"VOC transcriptions v2 - GLOBALISE\", https://hdl.handle.net/10622/LVXSBW, IISH Data Collection</li> </ul> <p>The project conveniently provides a file with pointers to all txt files in this dataset that we can download automatically. We are using <code>wget</code> to download the files. First the file with pointers, which we will use to download all txt files. This can take a while.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#pre-processing","title":"Pre-processing\u00b6","text":""},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#processing","title":"Processing\u00b6","text":"<p>Now that we have the data in a usable format, we can start processing it. We will use the Gensim library to train a Word2Vec model on the text data. For this, we first create a Corpus object that will be used to feed text to the model. We use a custom implementation of the <code>gensim.corpora.textcorpus.TextCorpus</code> class to now have a cutoff for the number of words in the vocabulary (standard settings).</p>"},{"location":"experiments/blacklab-search-interface-general-missives/","title":"BlackLab Search Interface for the General Missives of the VOC","text":"<p>Date: 2021 (pre-GLOBALISE) URL: https://corpora.ato.ivdnt.org/corpus-frontend/Missiven/search Status: Production People involved: Sophie Arnoult, Jesse de Does, Dirk Roorda, Jan Niestadt, Lodewijk Petram, Piek Vossen, Jessica den Oudsten, Dani\u00ebl Tuik</p> <p>A BlackLab search environment offers a new way to explore the General Missives of the Dutch East India Company (VOC). These reports, sent from Batavia (Jakarta) to the Dutch Republic between 1610 and 1795, are now accessible for in-depth research thanks to efforts within the CLARIAH project by a team from VU University, the Huygens Institute, and the Dutch Language Institute. Utilizing advanced OCR and Named Entity Recognition techniques<sup>1</sup>, the team enhanced these documents with metadata and structural elements, including annotations for entities like persons and locations.</p> <p>The BlackLab interface facilitates computational analysis and robust search capabilities of the enriched texts. Researchers can now perform complex syntactic searches or simple keyword queries, uncovering nuanced historical and linguistic insights. A slightly cleaner version of the same corpus is also available as a Text-Fabric resource.</p> <p> https://corpora.ato.ivdnt.org/corpus-frontend/Missiven/search</p> <p>The General Missives summarize the information contained in the Overgekomen Brieven en Papieren series of documents from the VOC archives that the GLOBALISE project aimes to unlock for in-depth research. The corpus available in the BlackLab environment is a selection of General Missives from the period 1610-1767 that was transcribed, edited and published in 14 (digital) book volumes by the Huygens Institute and its predecessors. The original volumes are also available online. </p> <p>Please note that the General Missives contain labels, characterizations and information about persons, actions and events that may be offensive and troubling to individuals and communities.</p> <ol> <li> <p>Sophie I. Arnoult, Lodewijk Petram, and Piek Vossen. 2021. Batavia asked for advice. Pretrained language models for Named Entity Recognition in historical texts. In Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 21\u201330, Punta Cana, Dominican Republic (online). Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.latechclfl-1.3 \u21a9</p> </li> </ol>"},{"location":"experiments/htr-viewer/","title":"GLOBALISE Transcriptions Viewer","text":"<p>Date: October 4, 2023 URL: https://transcriptions.globalise.huygens.knaw.nl/ Status: Prototype People involved: Sebastiaan van Daalen, Hayco de Jong, Bram Buitendijk, Hennie Brugman, Arno Bosse, Leon van Wissen, Lodewijk Petram</p> <p>The aim of the GLOBALISE project is to facilitate research with the Overgekomen Brieven en Papieren series of documents from the VOC archives. As a first step to reaching this goal, we generate transcriptions of the c. 5 million of handwritten pages made available by the Dutch National Archives using automatic transcription software. </p> <p>While we publish text files of the transcriptions on the GLOBALISE Dataverse, we also experiment with building an interface for easy searching and exploring the material. A first prototype can be accessed through the link below. Please share your feedback through our contact form. In the future, improved versions will be made available.</p> <p> https://transcriptions.globalise.huygens.knaw.nl/</p> <p>The collection of archival documents made available in the viewer comprises inventory numbers 1053-4454 and 7527-11024 from the VOC archives, National Archives, The Hague. The scans of the original documents (n=4,802,212) from the period 1610-1796 are available on the website of the National Archives.</p> <p>Please note that the transcriptions will contain errors. They have not been manually checked for accuracy or completeness. Some labels, characterizations and information about persons, actions and events may be offensive and troubling to individuals and communities. Be careful when relying on these transcriptions and be aware of their limitations.</p>"},{"location":"experiments/places-visualization/","title":"GLOBALISE Places Visualization","text":"<p>Date: May 2023 URL: https://globalise.shinyapps.io/mapping_places/ Status: Prototype People involved: Ruben Land</p> <p>Initially as an intern at the GLOBALISE project and now as a student assistant, Ruben Land is working on a dataset of places that occur in the Overgekomen Brieven en Papieren series of VOC documents. He uses R Shiny to create interactive visualizations of his work. These can be accessed by clicking the image below.</p> <p> https://globalise.shinyapps.io/mapping_places/</p>"},{"location":"experiments/skosmos-concept-browser/","title":"Thesaurus concepts browser","text":"<p>We\u2019re working on developing a GLOBALISE thesaurus with definitions of concepts that occur in the Overgekomen Brieven en Papieren series of VOC documents. A preliminary version of the thesaurus can be explored in our SKOSMOS environment.<sup>1</sup></p> <p>Please note that the thesaurus is constantly being improved and extended, and the the URIs in the current version are not stable.</p> <p></p> <ol> <li> <p>This demo is running the SKOSMOS software, developed by the National Library of Finland, to provide a user-friendly interface to our thesaurus. The SKOSMOS software is open source and available on GitHub.\u00a0\u21a9</p> </li> </ol>"},{"location":"experiments/text-fabric-general-missives/","title":"Text-Fabric Serialization of the General Missives of the VOC","text":"<p>Date: 2022 (pre-GLOBALISE) URL: https://clariah.github.io/wp6-missieven-search/text/index.html and https://github.com/CLARIAH/wp6-missieven/ Status: Demo People involved: Dirk Roorda, Sophie Arnoult, Lodewijk Petram, Piek Vossen, Jesse de Does, Jessica den Oudsten, Dani\u00ebl Tuik</p> <p>A Text-Fabric representation of the General Missives of the Dutch East India Company (VOC) offers a new way to explore and analyze these reports. The General Missives sent from Batavia (Jakarta) to the Dutch Republic between 1610 and 1795, are now accessible for in-depth research thanks to efforts within the CLARIAH project by a team from VU University, the Huygens Institute, and the Dutch Language Institute. Utilizing advanced OCR and Named Entity Recognition techniques <sup>1</sup>, the team enhanced these documents with metadata and structural elements, including annotations for entities like persons and locations.</p> <p>The Text-Fabric serialization of the enriched texts is especially suited for linguistic analysis with computational methods. Users can explore the materials in the Text-Fabric search interface or by using the Text-Fabric Python package. A slightly less cleaned version of the same corpus is also available in a BlackLab search environment.</p> <p> https://clariah.github.io/wp6-missieven-search/text/index.html</p> <p>The General Missives summarize the information contained in the Overgekomen Brieven en Papieren series of documents from the VOC archives that the GLOBALISE project aimes to unlock for in-depth research. The corpus available in the BlackLab environment is a selection of General Missives from the period 1610-1767 that was transcribed, edited and published in 14 (digital) book volumes by the Huygens Institute and its predecessors. The original volumes are also available online. </p> <p>Please note that the General Missives contain labels, characterizations and information about persons, actions and events that may be offensive and troubling to individuals and communities.</p> <ol> <li> <p>Sophie I. Arnoult, Lodewijk Petram, and Piek Vossen. 2021. Batavia asked for advice. Pretrained language models for Named Entity Recognition in historical texts. In Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 21\u201330, Punta Cana, Dominican Republic (online). Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.latechclfl-1.3 \u21a9</p> </li> </ol>"}]}